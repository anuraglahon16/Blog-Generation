{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRa\n",
    "* Model: DistilBERT\n",
    "* Evaluation approach: accuracy\n",
    "* Fine-tuning dataset:  Amazon Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1bc62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c800da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 109kB/s]\n",
      "config.json: 100%|██████████| 483/483 [00:00<00:00, 1.78MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.72MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 6.51MB/s]\n",
      "model.safetensors: 100%|██████████| 268M/268M [00:02<00:00, 114MB/s]  \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading readme: 100%|██████████| 6.81k/6.81k [00:00<00:00, 6.60MB/s]\n",
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   4%|▎         | 4.19M/117M [00:00<00:06, 17.9MB/s]\u001b[A\n",
      "Downloading data:  11%|█         | 12.6M/117M [00:00<00:02, 37.6MB/s]\u001b[A\n",
      "Downloading data:  18%|█▊        | 21.0M/117M [00:00<00:02, 48.0MB/s]\u001b[A\n",
      "Downloading data:  25%|██▌       | 29.4M/117M [00:00<00:01, 53.9MB/s]\u001b[A\n",
      "Downloading data:  32%|███▏      | 37.7M/117M [00:00<00:01, 58.1MB/s]\u001b[A\n",
      "Downloading data:  39%|███▉      | 46.1M/117M [00:00<00:01, 60.6MB/s]\u001b[A\n",
      "Downloading data:  46%|████▋     | 54.5M/117M [00:01<00:01, 62.5MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▎    | 62.9M/117M [00:01<00:00, 62.9MB/s]\u001b[A\n",
      "Downloading data:  61%|██████    | 71.3M/117M [00:01<00:00, 64.1MB/s]\u001b[A\n",
      "Downloading data:  68%|██████▊   | 79.7M/117M [00:01<00:00, 65.3MB/s]\u001b[A\n",
      "Downloading data:  75%|███████▌  | 88.1M/117M [00:01<00:00, 56.0MB/s]\u001b[A\n",
      "Downloading data:  82%|████████▏ | 96.5M/117M [00:01<00:00, 57.6MB/s]\u001b[A\n",
      "Downloading data:  89%|████████▉ | 105M/117M [00:01<00:00, 60.1MB/s] \u001b[A\n",
      "Downloading data: 100%|██████████| 117M/117M [00:02<00:00, 51.1MB/s]\u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:02<00:02,  2.31s/it]\n",
      "Downloading data:   0%|          | 0.00/260M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   2%|▏         | 4.19M/260M [00:00<00:08, 30.0MB/s]\u001b[A\n",
      "Downloading data:   5%|▍         | 12.6M/260M [00:00<00:06, 40.5MB/s]\u001b[A\n",
      "Downloading data:   8%|▊         | 21.0M/260M [00:00<00:06, 39.8MB/s]\u001b[A\n",
      "Downloading data:  11%|█▏        | 29.4M/260M [00:00<00:05, 39.8MB/s]\u001b[A\n",
      "Downloading data:  15%|█▍        | 37.7M/260M [00:00<00:04, 45.7MB/s]\u001b[A\n",
      "Downloading data:  18%|█▊        | 46.1M/260M [00:01<00:04, 50.2MB/s]\u001b[A\n",
      "Downloading data:  21%|██        | 54.5M/260M [00:01<00:03, 54.5MB/s]\u001b[A\n",
      "Downloading data:  24%|██▍       | 62.9M/260M [00:01<00:03, 56.1MB/s]\u001b[A\n",
      "Downloading data:  27%|██▋       | 71.3M/260M [00:01<00:03, 59.3MB/s]\u001b[A\n",
      "Downloading data:  31%|███       | 79.7M/260M [00:01<00:02, 60.3MB/s]\u001b[A\n",
      "Downloading data:  34%|███▍      | 88.1M/260M [00:01<00:02, 58.6MB/s]\u001b[A\n",
      "Downloading data:  37%|███▋      | 96.5M/260M [00:01<00:03, 49.6MB/s]\u001b[A\n",
      "Downloading data:  40%|████      | 105M/260M [00:02<00:03, 44.8MB/s] \u001b[A\n",
      "Downloading data:  44%|████▎     | 113M/260M [00:02<00:03, 44.4MB/s]\u001b[A\n",
      "Downloading data:  47%|████▋     | 122M/260M [00:02<00:02, 48.6MB/s]\u001b[A\n",
      "Downloading data:  50%|█████     | 130M/260M [00:02<00:02, 47.3MB/s]\u001b[A\n",
      "Downloading data:  53%|█████▎    | 138M/260M [00:02<00:02, 49.0MB/s]\u001b[A\n",
      "Downloading data:  57%|█████▋    | 147M/260M [00:03<00:02, 49.0MB/s]\u001b[A\n",
      "Downloading data:  60%|█████▉    | 155M/260M [00:03<00:02, 51.2MB/s]\u001b[A\n",
      "Downloading data:  63%|██████▎   | 164M/260M [00:03<00:01, 54.3MB/s]\u001b[A\n",
      "Downloading data:  66%|██████▌   | 172M/260M [00:03<00:01, 56.9MB/s]\u001b[A\n",
      "Downloading data:  69%|██████▉   | 180M/260M [00:03<00:01, 55.4MB/s]\u001b[A\n",
      "Downloading data:  73%|███████▎  | 189M/260M [00:03<00:01, 52.5MB/s]\u001b[A\n",
      "Downloading data:  76%|███████▌  | 197M/260M [00:03<00:01, 52.7MB/s]\u001b[A\n",
      "Downloading data:  79%|███████▉  | 206M/260M [00:04<00:01, 53.9MB/s]\u001b[A\n",
      "Downloading data:  82%|████████▏ | 214M/260M [00:04<00:00, 55.1MB/s]\u001b[A\n",
      "Downloading data:  86%|████████▌ | 222M/260M [00:04<00:00, 55.7MB/s]\u001b[A\n",
      "Downloading data:  89%|████████▉ | 231M/260M [00:04<00:00, 56.1MB/s]\u001b[A\n",
      "Downloading data:  92%|█████████▏| 239M/260M [00:04<00:00, 55.1MB/s]\u001b[A\n",
      "Downloading data:  95%|█████████▌| 247M/260M [00:04<00:00, 57.8MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 260M/260M [00:05<00:00, 44.6MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/258M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   2%|▏         | 4.19M/258M [00:00<00:09, 27.0MB/s]\u001b[A\n",
      "Downloading data:   5%|▍         | 12.6M/258M [00:00<00:11, 21.6MB/s]\u001b[A\n",
      "Downloading data:   8%|▊         | 21.0M/258M [00:00<00:07, 33.4MB/s]\u001b[A\n",
      "Downloading data:  11%|█▏        | 29.4M/258M [00:00<00:05, 41.5MB/s]\u001b[A\n",
      "Downloading data:  15%|█▍        | 37.7M/258M [00:00<00:04, 46.9MB/s]\u001b[A\n",
      "Downloading data:  18%|█▊        | 46.1M/258M [00:01<00:04, 47.1MB/s]\u001b[A\n",
      "Downloading data:  21%|██        | 54.5M/258M [00:01<00:04, 50.3MB/s]\u001b[A\n",
      "Downloading data:  24%|██▍       | 62.9M/258M [00:01<00:03, 49.1MB/s]\u001b[A\n",
      "Downloading data:  28%|██▊       | 71.3M/258M [00:01<00:03, 51.1MB/s]\u001b[A\n",
      "Downloading data:  31%|███       | 79.7M/258M [00:01<00:03, 52.9MB/s]\u001b[A\n",
      "Downloading data:  34%|███▍      | 88.1M/258M [00:01<00:03, 49.2MB/s]\u001b[A\n",
      "Downloading data:  37%|███▋      | 96.5M/258M [00:02<00:03, 51.8MB/s]\u001b[A\n",
      "Downloading data:  41%|████      | 105M/258M [00:02<00:02, 54.7MB/s] \u001b[A\n",
      "Downloading data:  44%|████▍     | 113M/258M [00:02<00:03, 46.3MB/s]\u001b[A\n",
      "Downloading data:  47%|████▋     | 122M/258M [00:02<00:02, 50.8MB/s]\u001b[A\n",
      "Downloading data:  50%|█████     | 130M/258M [00:02<00:02, 54.2MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▎    | 138M/258M [00:02<00:02, 56.1MB/s]\u001b[A\n",
      "Downloading data:  57%|█████▋    | 147M/258M [00:03<00:01, 56.3MB/s]\u001b[A\n",
      "Downloading data:  60%|██████    | 155M/258M [00:03<00:01, 57.2MB/s]\u001b[A\n",
      "Downloading data:  63%|██████▎   | 164M/258M [00:03<00:01, 55.8MB/s]\u001b[A\n",
      "Downloading data:  67%|██████▋   | 172M/258M [00:03<00:01, 58.9MB/s]\u001b[A\n",
      "Downloading data:  70%|██████▉   | 180M/258M [00:03<00:01, 60.3MB/s]\u001b[A\n",
      "Downloading data:  73%|███████▎  | 189M/258M [00:03<00:01, 58.6MB/s]\u001b[A\n",
      "Downloading data:  76%|███████▋  | 197M/258M [00:03<00:01, 58.8MB/s]\u001b[A\n",
      "Downloading data:  80%|███████▉  | 206M/258M [00:04<00:00, 56.2MB/s]\u001b[A\n",
      "Downloading data:  83%|████████▎ | 214M/258M [00:04<00:00, 53.8MB/s]\u001b[A\n",
      "Downloading data:  86%|████████▌ | 222M/258M [00:04<00:01, 30.7MB/s]\u001b[A\n",
      "Downloading data:  89%|████████▉ | 231M/258M [00:04<00:00, 36.3MB/s]\u001b[A\n",
      "Downloading data:  93%|█████████▎| 239M/258M [00:05<00:00, 41.3MB/s]\u001b[A\n",
      "Downloading data:  96%|█████████▌| 247M/258M [00:05<00:00, 45.1MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 258M/258M [00:06<00:00, 41.6MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/255M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   2%|▏         | 4.19M/255M [00:00<00:14, 17.7MB/s]\u001b[A\n",
      "Downloading data:   5%|▍         | 12.6M/255M [00:00<00:06, 38.5MB/s]\u001b[A\n",
      "Downloading data:   8%|▊         | 21.0M/255M [00:00<00:04, 49.1MB/s]\u001b[A\n",
      "Downloading data:  11%|█▏        | 29.4M/255M [00:00<00:04, 55.4MB/s]\u001b[A\n",
      "Downloading data:  15%|█▍        | 37.7M/255M [00:00<00:03, 55.5MB/s]\u001b[A\n",
      "Downloading data:  18%|█▊        | 46.1M/255M [00:00<00:03, 56.7MB/s]\u001b[A\n",
      "Downloading data:  21%|██▏       | 54.5M/255M [00:01<00:03, 57.8MB/s]\u001b[A\n",
      "Downloading data:  25%|██▍       | 62.9M/255M [00:01<00:03, 57.5MB/s]\u001b[A\n",
      "Downloading data:  28%|██▊       | 71.3M/255M [00:01<00:03, 57.7MB/s]\u001b[A\n",
      "Downloading data:  31%|███       | 79.7M/255M [00:01<00:02, 59.0MB/s]\u001b[A\n",
      "Downloading data:  34%|███▍      | 88.1M/255M [00:01<00:02, 57.7MB/s]\u001b[A\n",
      "Downloading data:  38%|███▊      | 96.5M/255M [00:01<00:02, 55.5MB/s]\u001b[A\n",
      "Downloading data:  41%|████      | 105M/255M [00:01<00:02, 56.5MB/s] \u001b[A\n",
      "Downloading data:  44%|████▍     | 113M/255M [00:02<00:02, 56.4MB/s]\u001b[A\n",
      "Downloading data:  48%|████▊     | 122M/255M [00:02<00:02, 56.1MB/s]\u001b[A\n",
      "Downloading data:  51%|█████     | 130M/255M [00:02<00:02, 58.5MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▍    | 138M/255M [00:02<00:01, 60.7MB/s]\u001b[A\n",
      "Downloading data:  57%|█████▋    | 147M/255M [00:02<00:01, 57.5MB/s]\u001b[A\n",
      "Downloading data:  61%|██████    | 155M/255M [00:02<00:01, 59.0MB/s]\u001b[A\n",
      "Downloading data:  64%|██████▍   | 164M/255M [00:02<00:01, 57.8MB/s]\u001b[A\n",
      "Downloading data:  67%|██████▋   | 172M/255M [00:03<00:01, 57.7MB/s]\u001b[A\n",
      "Downloading data:  71%|███████   | 180M/255M [00:03<00:01, 55.5MB/s]\u001b[A\n",
      "Downloading data:  74%|███████▍  | 189M/255M [00:03<00:01, 57.7MB/s]\u001b[A\n",
      "Downloading data:  77%|███████▋  | 197M/255M [00:03<00:00, 60.2MB/s]\u001b[A\n",
      "Downloading data:  80%|████████  | 206M/255M [00:03<00:00, 59.0MB/s]\u001b[A\n",
      "Downloading data:  84%|████████▎ | 214M/255M [00:03<00:00, 59.1MB/s]\u001b[A\n",
      "Downloading data:  87%|████████▋ | 222M/255M [00:03<00:00, 54.7MB/s]\u001b[A\n",
      "Downloading data:  90%|█████████ | 231M/255M [00:04<00:00, 57.6MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████▎| 239M/255M [00:04<00:00, 59.0MB/s]\u001b[A\n",
      "Downloading data:  97%|█████████▋| 247M/255M [00:04<00:00, 59.0MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 255M/255M [00:05<00:00, 47.0MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/254M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   2%|▏         | 4.19M/254M [00:00<00:09, 26.4MB/s]\u001b[A\n",
      "Downloading data:   5%|▍         | 12.6M/254M [00:00<00:08, 28.7MB/s]\u001b[A\n",
      "Downloading data:   8%|▊         | 21.0M/254M [00:00<00:06, 37.7MB/s]\u001b[A\n",
      "Downloading data:  12%|█▏        | 29.4M/254M [00:00<00:05, 44.5MB/s]\u001b[A\n",
      "Downloading data:  15%|█▍        | 37.7M/254M [00:00<00:04, 49.1MB/s]\u001b[A\n",
      "Downloading data:  18%|█▊        | 46.1M/254M [00:01<00:04, 47.3MB/s]\u001b[A\n",
      "Downloading data:  21%|██▏       | 54.5M/254M [00:01<00:04, 49.4MB/s]\u001b[A\n",
      "Downloading data:  25%|██▍       | 62.9M/254M [00:01<00:03, 52.8MB/s]\u001b[A\n",
      "Downloading data:  28%|██▊       | 71.3M/254M [00:01<00:03, 54.0MB/s]\u001b[A\n",
      "Downloading data:  31%|███▏      | 79.7M/254M [00:01<00:03, 57.2MB/s]\u001b[A\n",
      "Downloading data:  35%|███▍      | 88.1M/254M [00:01<00:02, 59.2MB/s]\u001b[A\n",
      "Downloading data:  38%|███▊      | 96.5M/254M [00:01<00:02, 58.0MB/s]\u001b[A\n",
      "Downloading data:  41%|████      | 105M/254M [00:02<00:02, 56.4MB/s] \u001b[A\n",
      "Downloading data:  45%|████▍     | 113M/254M [00:02<00:02, 57.9MB/s]\u001b[A\n",
      "Downloading data:  48%|████▊     | 122M/254M [00:02<00:02, 59.5MB/s]\u001b[A\n",
      "Downloading data:  51%|█████     | 130M/254M [00:02<00:02, 61.2MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▍    | 138M/254M [00:02<00:01, 62.5MB/s]\u001b[A\n",
      "Downloading data:  58%|█████▊    | 147M/254M [00:02<00:01, 59.2MB/s]\u001b[A\n",
      "Downloading data:  61%|██████    | 155M/254M [00:02<00:01, 61.8MB/s]\u001b[A\n",
      "Downloading data:  64%|██████▍   | 164M/254M [00:03<00:01, 61.5MB/s]\u001b[A\n",
      "Downloading data:  68%|██████▊   | 172M/254M [00:03<00:01, 60.6MB/s]\u001b[A\n",
      "Downloading data:  71%|███████   | 180M/254M [00:03<00:01, 59.6MB/s]\u001b[A\n",
      "Downloading data:  74%|███████▍  | 189M/254M [00:03<00:01, 58.4MB/s]\u001b[A\n",
      "Downloading data:  77%|███████▋  | 197M/254M [00:03<00:00, 60.6MB/s]\u001b[A\n",
      "Downloading data:  81%|████████  | 206M/254M [00:03<00:00, 61.4MB/s]\u001b[A\n",
      "Downloading data:  84%|████████▍ | 214M/254M [00:03<00:00, 60.8MB/s]\u001b[A\n",
      "Downloading data:  87%|████████▋ | 222M/254M [00:04<00:00, 58.5MB/s]\u001b[A\n",
      "Downloading data:  91%|█████████ | 231M/254M [00:04<00:00, 57.5MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████▍| 239M/254M [00:04<00:00, 58.8MB/s]\u001b[A\n",
      "Downloading data:  97%|█████████▋| 247M/254M [00:04<00:00, 59.6MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 254M/254M [00:05<00:00, 46.6MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 2/2 [00:25<00:00, 12.66s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 589.29it/s]\n",
      "Generating test split: 100%|██████████| 400000/400000 [00:00<00:00, 463056.45 examples/s]\n",
      "Generating train split: 100%|██████████| 3600000/3600000 [00:07<00:00, 450888.29 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2043.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# foundation model (DistilBERT) \n",
    "model_name = \"distilbert-base-uncased\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"amazon_polarity\", split=\"train[:500]\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"content\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Split the tokenized dataset into training and evaluation sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(tokenized_dataset[\"input_ids\"]) * split_ratio)\n",
    "\n",
    "train_dataset = {\n",
    "    \"input_ids\": tokenized_dataset[\"input_ids\"][:split_index],\n",
    "    \"attention_mask\": tokenized_dataset[\"attention_mask\"][:split_index],\n",
    "    \"label\": tokenized_dataset[\"label\"][:split_index],\n",
    "}\n",
    "eval_dataset = {\n",
    "    \"input_ids\": tokenized_dataset[\"input_ids\"][split_index:],\n",
    "    \"attention_mask\": tokenized_dataset[\"attention_mask\"][split_index:],\n",
    "    \"label\": tokenized_dataset[\"label\"][split_index:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to PyTorch Dataset objects\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"label\": self.label[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = CustomDataset(**train_dataset)\n",
    "eval_dataset = CustomDataset(**eval_dataset)\n",
    "\n",
    "# Define the evaluation function for the Trainer\n",
    "def compute_metrics(p):\n",
    "    return {\"accuracy\": (p.predictions.argmax(axis=1) == p.label_ids).mean()}\n",
    "\n",
    "# Trainer for foundation model evaluation\n",
    "training_args_foundation = TrainingArguments(\n",
    "    output_dir=\"./foundation_output\",\n",
    "    per_device_eval_batch_size=8,\n",
    ")\n",
    "trainer_foundation = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_foundation,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "foundation_results = trainer_foundation.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# EFT model \n",
    "peft_model = AutoModelForSequenceClassification.from_pretrained(model_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments for PEFT\n",
    "training_args_peft = TrainingArguments(\n",
    "    output_dir=\"./peft_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for PEFT\n",
    "trainer_peft = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args_peft,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.376335</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.655900</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.373182</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412556</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.422427</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Model Results: 0.71\n",
      "PEFT Model Results: 0.87\n"
     ]
    }
   ],
   "source": [
    "trainer_peft.train()\n",
    "\n",
    "# Evaluate the PEFT model \n",
    "peft_results = trainer_peft.evaluate()\n",
    "\n",
    "# Compare results with the foundation model's performance\n",
    "print(\"Foundation Model Results:\", foundation_results['eval_accuracy'])\n",
    "print(\"PEFT Model Results:\", peft_results['eval_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules ['q', 'v'] not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 85\u001b[0m\n\u001b[1;32m     77\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     78\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[1;32m     79\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m     80\u001b[0m     bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Create a PEFT model using the foundation model and PEFT config\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Define the training arguments for PEFT\u001b[39;00m\n\u001b[1;32m     88\u001b[0m training_args_peft \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     89\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./peft_output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     90\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     94\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/mapping.py:103\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    100\u001b[0m peft_config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeftModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    105\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:111\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m peft_config\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mPEFT_TYPE_TO_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:274\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:88\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:222\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_and_replace(peft_config, adapter_name, target, target_name, parent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptionnal_kwargs)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_only_adapters_as_trainable()\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39minference_mode:\n",
      "\u001b[0;31mValueError\u001b[0m: Target modules ['q', 'v'] not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Choose your foundation model (e.g., DistilBERT) and load it\n",
    "model_name = \"distilbert-base-uncased\"  # Change this to your chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load an appropriate dataset for sequence classification from Hugging Face datasets library\n",
    "# (e.g., Amazon Customer Reviews as mentioned earlier)\n",
    "# Make sure it is small enough for the Udacity Workspace\n",
    "# You can load the dataset using the datasets library\n",
    "dataset = load_dataset(\"amazon_polarity\", split=\"train[:500]\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"content\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Split the tokenized dataset into training and evaluation sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(tokenized_dataset[\"input_ids\"]) * split_ratio)\n",
    "\n",
    "train_dataset = {\n",
    "    \"input_ids\": tokenized_dataset[\"input_ids\"][:split_index],\n",
    "    \"attention_mask\": tokenized_dataset[\"attention_mask\"][:split_index],\n",
    "    \"label\": tokenized_dataset[\"label\"][:split_index],\n",
    "}\n",
    "eval_dataset = {\n",
    "    \"input_ids\": tokenized_dataset[\"input_ids\"][split_index:],\n",
    "    \"attention_mask\": tokenized_dataset[\"attention_mask\"][split_index:],\n",
    "    \"label\": tokenized_dataset[\"label\"][split_index:],\n",
    "}\n",
    "\n",
    "# Convert datasets to PyTorch Dataset objects\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"label\": self.label[idx],\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(**train_dataset)\n",
    "eval_dataset = CustomDataset(**eval_dataset)\n",
    "\n",
    "# Define the evaluation function for the Trainer\n",
    "def compute_metrics(p):\n",
    "    return {\"accuracy\": (p.predictions.argmax(axis=1) == p.label_ids).mean()}\n",
    "\n",
    "# Create a Trainer for foundation model evaluation\n",
    "training_args_foundation = TrainingArguments(\n",
    "    output_dir=\"./foundation_output\",\n",
    "    per_device_eval_batch_size=8,\n",
    ")\n",
    "trainer_foundation = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_foundation,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "foundation_results = trainer_foundation.evaluate()\n",
    "\n",
    "# Create a PEFT config with LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=4, \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q\", \"v\"],\n",
    ")\n",
    "\n",
    "# Create a PEFT model using the foundation model and PEFT config\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Define the training arguments for PEFT\n",
    "training_args_peft = TrainingArguments(\n",
    "    output_dir=\"./peft_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    ")\n",
    "\n",
    "# Create a Trainer for PEFT\n",
    "trainer_peft = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args_peft,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "trainer_peft.train()\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Save the trained PEFT model\n",
    "peft_model.save_pretrained(\"bert-lora\")\n",
    "\n",
    "# Evaluate the PEFT model on the same dataset\n",
    "peft_results = trainer_peft.evaluate()\n",
    "\n",
    "# Compare the results with the foundation model's performance\n",
    "print(\"Foundation Model Results:\", foundation_results['eval_accuracy'])\n",
    "print(\"PEFT Model Results:\", peft_results['eval_accuracy'])\n",
    "\n",
    "# Load the PEFT model for inference\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "loaded_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"bert-lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
