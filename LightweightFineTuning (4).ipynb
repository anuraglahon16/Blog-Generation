{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRa\n",
    "* Model: bert-base-cased\n",
    "* Evaluation approach: accuracy\n",
    "* Fine-tuning dataset:  imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1bc62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c800da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 50000/50000 [00:54<00:00, 911.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# foundation model (DistilBERT) \n",
    "model_name = \"bert-base-cased\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Split the tokenized dataset into training and evaluation sets\n",
    "#split_ratio = 0.8\n",
    "#split_index = int(len(tokenized_dataset[\"input_ids\"]) * split_ratio)\n",
    "\n",
    "#train_dataset = {\n",
    " #   \"input_ids\": tokenized_dataset[\"input_ids\"][:split_index],\n",
    "  #  \"attention_mask\": tokenized_dataset[\"attention_mask\"][:split_index],\n",
    "   # \"label\": tokenized_dataset[\"label\"][:split_index],\n",
    "#}\n",
    "#eval_dataset = {\n",
    " #   \"input_ids\": tokenized_dataset[\"input_ids\"][split_index:],\n",
    "  #  \"attention_mask\": tokenized_dataset[\"attention_mask\"][split_index:],\n",
    "   # \"label\": tokenized_dataset[\"label\"][split_index:],\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to PyTorch Dataset objects\n",
    "#class CustomDataset(Dataset):\n",
    " #   def __init__(self, input_ids, attention_mask, label):\n",
    "  #      self.input_ids = input_ids\n",
    "   #     self.attention_mask = attention_mask\n",
    "    #    self.label = label\n",
    "\n",
    "    #def __len__(self):\n",
    "     #   return len(self.input_ids)\n",
    "\n",
    "    #def __getitem__(self, idx):\n",
    "     #   return {\n",
    "      #      \"input_ids\": self.input_ids[idx],\n",
    "       #     \"attention_mask\": self.attention_mask[idx],\n",
    "        #    \"label\": self.label[idx],\n",
    "       # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7933e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import evaluate\n",
    "#!pip install evaluate\n",
    "#metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/student/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_dataset = CustomDataset(**train_dataset)\n",
    "#eval_dataset = CustomDataset(**eval_dataset)\n",
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "# Define the evaluation function for the Trainer\n",
    "#def compute_metrics(p):\n",
    "#    return {\"accuracy\": (p.predictions.argmax(axis=1) == p.label_ids).mean()}\n",
    "import numpy as np\n",
    "!pip install scikit-learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Trainer for foundation model evaluation\n",
    "training_args_foundation = TrainingArguments(\n",
    "    output_dir=\"./foundation_output\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15\n",
    "   \n",
    ")\n",
    "trainer_foundation = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_foundation,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "foundation_results = trainer_foundation.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ff1197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6994965672492981,\n",
       " 'eval_accuracy': 0.509,\n",
       " 'eval_runtime': 32.3415,\n",
       " 'eval_samples_per_second': 30.92,\n",
       " 'eval_steps_per_second': 3.865}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d07043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_eval_results = trainer_foundation.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EFT model \n",
    "#peft_model = AutoModelForSequenceClassification.from_pretrained(model_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments for PEFT\n",
    "#training_args_peft = TrainingArguments(\n",
    " #   output_dir=\"./peft_output\",\n",
    " #   evaluation_strategy=\"epoch\",\n",
    " #   learning_rate=2e-5,\n",
    "  #  per_device_train_batch_size=8,\n",
    "  #  num_train_epochs=5,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for PEFT\n",
    "#trainer_peft = Trainer(\n",
    " #   model=peft_model,\n",
    "#    args=training_args_peft,\n",
    "#    compute_metrics=compute_metrics,\n",
    " #   train_dataset=train_dataset,\n",
    " #   eval_dataset=eval_dataset,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer_peft.train()\n",
    "\n",
    "# Evaluate the PEFT model \n",
    "#peft_results = trainer_peft.evaluate()\n",
    "\n",
    "# Compare results with the foundation model's performance\n",
    "#print(\"Foundation Model Results:\", foundation_results['eval_accuracy'])\n",
    "#print(\"PEFT Model Results:\", peft_results['eval_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "445b13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "395fd15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:27<00:00, 913.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c5e2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "#small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9ad236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=1, lora_alpha=1, lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0be575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased', \n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e863fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "251d4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"bert-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20d1d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    return {\"accuracy\": (p.predictions.argmax(axis=1) == p.label_ids).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb86af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def compute_metrics(eval_pred):\n",
    " #   logits, labels = eval_pred\n",
    " #   predictions = np.argmax(logits, axis=-1)\n",
    " #   return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fbe39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",\n",
    "                                 num_train_epochs=15,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07d6d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset ,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f5d3a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 26:26, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686995</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.680050</td>\n",
       "      <td>0.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673739</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.663032</td>\n",
       "      <td>0.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.647790</td>\n",
       "      <td>0.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.620775</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.591044</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.643700</td>\n",
       "      <td>0.563524</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.643700</td>\n",
       "      <td>0.541664</td>\n",
       "      <td>0.731000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.643700</td>\n",
       "      <td>0.524831</td>\n",
       "      <td>0.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.643700</td>\n",
       "      <td>0.512873</td>\n",
       "      <td>0.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.501831</td>\n",
       "      <td>0.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.495847</td>\n",
       "      <td>0.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.492035</td>\n",
       "      <td>0.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.490828</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=0.6053807535807292, metrics={'train_runtime': 1587.117, 'train_samples_per_second': 9.451, 'train_steps_per_second': 1.181, 'total_flos': 3948435394560000.0, 'train_loss': 0.6053807535807292, 'epoch': 15.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43a3edcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5dfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63bc4d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Pre-trained and PEFT Models:\n",
      "Pre-trained Model: 0.509\n",
      "PEFT Model: 0.765\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparison of Pre-trained and PEFT Models:\")\n",
    "print(\"Pre-trained Model:\", foundation_eval_results['eval_accuracy'])\n",
    "print(\"PEFT Model:\", peft_eval_results['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f35149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323a178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9300c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
